# FastAPI Server для Llama.cpp

Этот проект представляет собой FastAPI сервер для работы с моделями Llama через llama-cpp-python.

## Установка

1. Установите зависимости:
```bash
pip install -r requirements.txt
```

2. Настройте переменные окружения в файле `.env`:
```env
MODEL_PATH=./path/to/your/model.gguf
N_CTX=4096
N_THREADS=16
```

## Использование

1. Убедитесь, что у вас есть модель в формате GGUF
2. Укажите правильный путь к модели в файле `.env`
3. Запустите сервер:

```bash
python Main.py
```

Сервер будет доступен по адресу: http://localhost:12000

## API Документация

После запуска сервера документация API будет доступна по адресу:
- Swagger UI: http://localhost:12000/docs
- ReDoc: http://localhost:12000/redoc

## Исправленные проблемы

### Основная ошибка
✅ **Исправлена ошибка с create_app()**: Функция `create_app()` теперь использует правильные параметры `ModelSettings` и `ServerSettings` вместо передачи экземпляра Llama напрямую.

### Дополнительные улучшения
1. ✅ Добавлена проверка существования файла модели
2. ✅ Добавлена валидация переменных окружения
3. ✅ Добавлена обработка ошибок при создании настроек
4. ✅ Добавлен fallback для chat_format при ошибках
5. ✅ Добавлен код для запуска сервера
6. ✅ Улучшена структура кода с разделением на функции
7. ✅ Добавлены информативные сообщения об ошибках
8. ✅ Настроены правильные параметры сервера

## Конфигурация

- `MODEL_PATH`: Путь к файлу модели в формате GGUF
- `N_CTX`: Размер контекста (по умолчанию 4096)
- `N_THREADS`: Количество потоков для обработки (по умолчанию 16)